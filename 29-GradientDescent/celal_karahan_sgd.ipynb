{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kütüphanelerin import Edilmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn Veri Seti Oluşturma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "x, y = make_regression(n_samples=100, n_features=1, noise=1, n_targets=1,random_state=42)\n",
    "\n",
    "x = np.array(x)\n",
    "y = np.array(y).reshape(-1, 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent Fonksiyonu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(x, y, theta0, theta1, learning_rate, iterations, threshold):\n",
    "    #stochastic gd nin farkı her iterasyonda veri setimizin sadece bir örneğini kullanarak parametreleri güncelleyeceğiz\n",
    "    m = len(x)\n",
    "    theta_history = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "        for j in range(m):\n",
    "            #sadece veri setimizin j örneğini hesaplayarak parametreleri hesapla\n",
    "            h = theta0 + theta1 * x[j]\n",
    "            error = h - y[j]\n",
    "        \n",
    "            theta0 = theta0 - learning_rate * error\n",
    "            theta1 = theta1 - learning_rate * error * x[j]\n",
    "\n",
    "        #hatanın thereshold dan küçük olup olmadığının kontrolü\n",
    "        error_sum = np.sum((theta0 + theta1 * x - y) ** 2)\n",
    "        if error_sum < threshold:\n",
    "            print(f\"Threshold ({threshold}) değeri aşıldı. İterasyon: {i + 1}\")\n",
    "            break\n",
    "        \n",
    "        theta_history.append((theta0, theta1))\n",
    "\n",
    "    return theta_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Karşılaştırma için Gradient Descent Algoritması"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, theta0, theta1, learning_rate, iterations, threshold):\n",
    "    m = len(x)\n",
    "    theta_history = []\n",
    "    for i in range(iterations):\n",
    "        # Hesapla\n",
    "        h = theta0 + theta1 * x\n",
    "        error = h - y\n",
    "        \n",
    "        # Gradient descent adımları\n",
    "        theta0 = theta0 - (learning_rate/m) * sum(error)\n",
    "        theta1 = theta1 - (learning_rate/m) * sum(error * x)\n",
    "        \n",
    "        error_sum = np.sum(error ** 2)  # Hata karesinin toplamını hesapla\n",
    "        if error_sum < threshold:\n",
    "            print(f\"Threshold ({threshold}) değeri aşıldı. İterasyon: {i + 1}\")\n",
    "            break\n",
    "        \n",
    "        #print(f\"Iteration {i + 1}: theta0 = {theta0}, theta1 = {theta1}\")\n",
    "        \n",
    "        theta_history.append((theta0, theta1))\n",
    "    return theta_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametre Değerleri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_theta0 = 1\n",
    "initial_theta1 = 1\n",
    "\n",
    "learning_rate = 0.01\n",
    "iterations = 1000\n",
    "threshold = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold (100) değeri aşıldı. İterasyon: 6\n",
      "Final theta0: [-0.10644024]\n",
      "Final theta1: [41.34659587]\n"
     ]
    }
   ],
   "source": [
    "# Görselleştirme amacıyla bütün iterasyon aşamalarındaki tetha değerleri liste olarak tutuluyor\n",
    "theta_history = stochastic_gradient_descent(x, y, initial_theta0, initial_theta1, learning_rate, iterations, threshold)\n",
    "\n",
    "final_theta0 = theta_history[(len(theta_history)-1)][0]\n",
    "final_theta1 = theta_history[(len(theta_history)-1)][1]\n",
    "\n",
    "print(f\"Final theta0: {final_theta0}\")\n",
    "print(f\"Final theta1: {final_theta1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold (100) değeri aşıldı. İterasyon: 545\n",
      "Final theta0: [-0.05236613]\n",
      "Final theta1: [41.51208528]\n"
     ]
    }
   ],
   "source": [
    "theta_history = gradient_descent(x, y, initial_theta0, initial_theta1, learning_rate, iterations, threshold)\n",
    "\n",
    "final_theta0 = theta_history[(len(theta_history)-1)][0]\n",
    "final_theta1 = theta_history[(len(theta_history)-1)][1]\n",
    "\n",
    "print(f\"Final theta0: {final_theta0}\")\n",
    "print(f\"Final theta1: {final_theta1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rapor\n",
    "Bu rapor Stochastic Gradient Descent (SGD)  Gradient Descent (GD) algoritmalarını kullanarak gerçekleştirilen regresyon modelinin eğitimi üzerine odaklanmaktadır. İki algoritma da bir eğitim eşiği (threshold) belirlenene kadar çalıştırıldı.\n",
    "\n",
    "1. Stochastic Gradient Descent (SGD):\n",
    "\n",
    "Threshold (Eşik) Değeri: 100\n",
    "İterasyon Sayısı: 6\n",
    "Final Theta0: -0.10644024\n",
    "Final Theta1: 41.34659587\n",
    "\n",
    "2. Batch Gradient Descent (BGD):\n",
    "\n",
    "Threshold (Eşik) Değeri: 100\n",
    "İterasyon Sayısı: 545\n",
    "Final Theta0: -0.05236613\n",
    "Final Theta1: 41.51208528\n",
    "\n",
    "Stochastic Gradient Descent algoritması, belirlenen eşik değerine daha hızlı ulaşarak %99'a yakın bir iterasyon sayısında düşüş sağlamıştır.Ancak bu hız daha rastgele güncellemelerin neden olduğu dalgalanmalarla birlikte gelir ve bu durum modelin hız ve kararlılığını etkileyebilir. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
